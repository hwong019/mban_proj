{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import libraries and dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\samue\\Downloads\\bank-additional-full.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Preliminary view of the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below histogram shows that the target value is imbalance and hence sampling technique need to be applied in the model design.\n",
    "sns.histplot(data=df, x='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # bank client data:\n",
    "   1 - age (numeric)\n",
    "   2 - job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n",
    "   3 - marital : marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n",
    "   4 - education (categorical: \"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\")\n",
    "   5 - default: has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   6 - housing: has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   7 - loan: has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   8 - contact: contact communication type (categorical: \"cellular\",\"telephone\") \n",
    "   9 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "  10 - day_of_week: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n",
    "  11 - duration: last contact duration, in seconds (numeric).\n",
    "  12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "  13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "  14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "  15 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n",
    "  16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "  17 - cons.price.idx: consumer price index - monthly indicator (numeric)     \n",
    "  18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)     \n",
    "  19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "  20 - nr.employed: number of employees - quarterly indicator (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any duplicate data and drop them\n",
    "df[df.duplicated()]\n",
    "\n",
    "# Proceed to drop due to immateiral number of records (<1%)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features between catgorical and numerical\n",
    "categorical = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "numerical = ['age','duration', 'campaign','pdays','previous','emp.var.rate','cons.conf.idx','euribor3m','nr.employed']\n",
    "target = 'y'\n",
    "\n",
    "# Check the number of 'unknown variable'\n",
    "for i in categorical:\n",
    "    print(i)\n",
    "    print(len(df[df[i]=='unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the unknown job data would have lots of positive target value\n",
    "sns.histplot(data=df, x='job', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the unknown marital data would have lots of positive target value\n",
    "sns.histplot(data=df, x='marital', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown value for job and marital are immaterial, hence proposed to drop them\n",
    "# Default, housing and loan are impute as no first. For default column, it will be dropped subsequently due to limited case of default.\n",
    "# education remain as unknown category\n",
    "\n",
    "df.drop(df[(df['job'] == 'unknown') | (df['marital'] == 'unknown')].index, inplace=True)\n",
    "df['default'] = df['default'].apply(lambda x: 'no' if x == 'unknown' else x)\n",
    "df['housing'] = df['housing'].apply(lambda x: 'no' if x == 'unknown' else x)\n",
    "df['loan'] = df['loan'].apply(lambda x: 'no' if x == 'unknown' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for education\n",
    "# summarize the most frequent education level by job\n",
    "job_mapping = df.groupby('job')['education'].apply(lambda x: x.mode().iloc[0])\n",
    "# transform the summary to dictionary\n",
    "job_mapping_dict = job_mapping.to_dict()\n",
    "# apply the mapping to the unknown value\n",
    "df['education'] = df.apply(lambda row: job_mapping_dict[row['job']] if (row['education'] == 'unknown') else row['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the imputation of education level\n",
    "sns.histplot(data=df, x='education', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the numerical details\n",
    "df[numerical].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['campaign']>10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='pdays', hue='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='previous', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duration since it is not known until the call was done\n",
    "# clear the outliers of campaign since it is not reasonable to call mutliple times in the same campaign (use 10 as a reference based on mean + 3 s.d.)\n",
    "# drop the pdays column since majority of the value is 999\n",
    "\n",
    "df.drop(['duration'], axis=1, inplace=True)\n",
    "df.drop((df[df['campaign'] > (df['campaign'].mean() + 3 * np.std(df['campaign']))]).index, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the list of numerical value\n",
    "numerical = ['campaign','previous','pdays','emp.var.rate','cons.conf.idx','euribor3m','nr.employed']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the relationship between emp.var.rate and nr.employed\n",
    "sns.lmplot(data=df, x='emp.var.rate', y='nr.employed')\n",
    "np.corrcoef(df['emp.var.rate'],df['nr.employed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high relationship between nr.employed and emp.var.rate, hence suggest to remove nr.employed to avoid duplication of features\n",
    "df.drop(['nr.employed'], axis=1, inplace=True)\n",
    "numerical = ['age','campaign','previous','pdays','emp.var.rate','cons.conf.idx','euribor3m']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram for categorical variable\n",
    "\n",
    "for var in categorical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=var, data=df, hue=target, palette='muted', alpha=0.7)\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Stacked Histogram for {var} based on Target')\n",
    "    plt.legend(title='Target', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis indicate that the success rate has less dependency on the day_of_week and existence of housing loan as they're distributed uniformly acorss the categorical values. Remove default columns since there is only three default cases.  Further, if the clients default, the bank will be benefited from taking deposits to them to reduce the overall credit exposures. Hence, default is less relevant in the term desposit subscription scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in numerical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=target, data=df, y=var)\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Boxplot for {var} based on Target')\n",
    "    plt.legend(title='Target', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the imbalance of target variable (11% of the data only)\n",
    "\n",
    "sns.histplot(data=df, x='y')\n",
    "(df['y']=='yes').sum() / ((df['y']=='yes').sum() + (df['y']=='no').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see distribution for all numeric variables\n",
    "df.hist(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a graph to see the corrlations between variables\n",
    "sns.heatmap(df[numerical].corr(), annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heatmap shows that there is high correlation between emp.var.rate and euribor3m.  Thus, we'll incldue either one of them into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed exploration\n",
    "sns.pairplot(df, hue=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below codes is to check the impact of removing certain unimprotant features based on the exploratory analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vairous liabraries for machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results when including full list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','pdays','emp.var.rate','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','housing','loan','contact','month','day_of_week','poutcome']\n",
    "target = 'y'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)\n",
    "\n",
    "# Build the LogisticRegression model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results when removing empvar (cumulative from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','pdays','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','housing','loan','contact','month','day_of_week','poutcome']\n",
    "target = 'y'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)\n",
    "\n",
    "# Build the LogisticRegression model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results when removing housing and day of week (cumulative from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','pdays','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','loan','contact','month','poutcome']\n",
    "target = 'y'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)\n",
    "\n",
    "# Build the LogisticRegression model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results when pdays (cumulative from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','loan','contact','month','poutcome']\n",
    "target = 'y'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)\n",
    "\n",
    "# Build the LogisticRegression model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is the actual model building after confirming the list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vairous liabraries for machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, classification_report, fbeta_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','loan','contact','month','poutcome']\n",
    "target = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LogisticRegression model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_lr))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_lr, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_knn = knn_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_knn))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_knn, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "svc_linear_model = SVC(kernel='linear')\n",
    "svc_linear_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svc_linear = svc_linear_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc_linear))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svc_linear))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_svc_linear))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_svc_linear, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC: Run-time 6mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "svc_model_rbf = SVC(kernel='rbf', gamma=0.1)\n",
    "svc_model_rbf.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svc_rbf = svc_model_rbf.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc_rbf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svc_rbf))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_svc_rbf))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_svc_rbf, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF SVC: Runtime 3mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.01)\n",
    "gbc_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gbc = gbc_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gbc))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gbc))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gbc))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_gbc, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "gbc_model_2 = GradientBoostingClassifier(n_estimators=2000, learning_rate=0.001, max_depth=2, subsample=0.8)\n",
    "gbc_model_2.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gbc_2 = gbc_model_2.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gbc_2))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gbc_2))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gbc_2))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_gbc, beta=2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform randomsearch\n",
    "params_gbc = {'n_estimators': list(range(1000,3000)),'learning_rate': [0.01,0.001],'max_depth': list(range(1,5)),'subsample': [0.5,0.8,1]}\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "randomsearch_gbc = RandomizedSearchCV(gbc_model_2, params_gbc, cv=5, n_iter=500, n_jobs=-1, scoring=ftwo_scorer, random_state=1234, verbose=2)\n",
    "randomsearch_gbc.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "print(\"Best parameters from RandomSearch: \", randomsearch_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_transformed.toarray(), y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_nb = nb_model.predict(X_test_transformed.toarray())\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_nb))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_nb, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Perform randomsearch\n",
    "# params_gbc = {'n_estimators': list(range(50,100)),'learning_rate': [0.1,0.2],'max_depth': list(range(1,5))}\n",
    "\n",
    "# from sklearn.metrics import fbeta_score, make_scorer\n",
    "# ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# randomsearch_gbc = RandomizedSearchCV(gbc_model, params_gbc, cv=5, n_iter=300, scoring=ftwo_scorer, random_state=1234)\n",
    "# randomsearch_gbc.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# print(\"Best parameters from RandomSearch: \", randomsearch_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is to explore the hyperparameter tunning for GBC on n_estimators, learning rate and max depth. n_iternation is set at 300 since we want to cover majority (~75%) of all the combinations (51*2*4 = 408). The code are commented due to the long runtime (~160mins) and the results is pasted as below.\n",
    "\n",
    "Best parameters from RandomSearch:  {'n_estimators': 92, 'max_depth': 4, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "gbc_model_best = GradientBoostingClassifier(n_estimators=92, max_depth=4, learning_rate=0.2)\n",
    "gbc_model_best.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gbc_best = gbc_model_best.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gbc_best))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gbc_best))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gbc_best))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_gbc_best, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores_gbc = cross_val_score(gbc_model, X_train_transformed, y_train_transformed, cv=5, scoring=ftwo_scorer)\n",
    "\n",
    "print(\"Cross-validation scores for gbc: \", cv_scores_gbc)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_gbc = gbc_model.predict(X_test_transformed)\n",
    "print(\"Test accuracy for gbc: \", recall_score(y_test, y_pred_gbc))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gbc))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_gbc, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores_gbc_best = cross_val_score(gbc_model_best, X_train_transformed, y_train_transformed, cv=5, scoring=ftwo_scorer)\n",
    "\n",
    "print(\"Cross-validation scores for gbc: \", cv_scores_gbc_best)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_gbc_best = gbc_model_best.predict(X_test_transformed)\n",
    "print(\"Test accuracy for gbc: \", recall_score(y_test, y_pred_gbc_best))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gbc_best))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_gbc_best, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rfc = rfc_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rfc))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rfc))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_rfc))\n",
    "print(\"F2:\", fbeta_score(y_test, y_pred_rfc, beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is to draw the detection error tradeoff curve which shows the false negative on the y-axis. The model performs the best if the curve is closer to the bottom-left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import det_curve\n",
    "\n",
    "def plot_det_curve_for_model(model, X_test, y_test, model_name):\n",
    "    fpr, fnr, thresholds = det_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    plt.plot(fpr, fnr, label=model_name)\n",
    "\n",
    "# Assuming you have a list of models named 'models_list', where each element is a tuple (model, model_name)\n",
    "models_list = [(gbc_model_best, \"GBC_RandomSearchCV\"), (gbc_model, \"GBC_default\"), (lr_model, \"LogistricRegression\"), \n",
    "               (knn_model, \"KNN_model\"), (nb_model, \"NaiveBayes_model\")]\n",
    "\n",
    "# Plotting the DET curves for all models\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model, model_name in models_list:\n",
    "    plot_det_curve_for_model(model, X_test_transformed, y_test, model_name)\n",
    "\n",
    "# Additional customizations for the plot\n",
    "plt.title(\"Detection Error Tradeoff (DET) curves\")\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"False Negative Rate (FNR)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
