{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: import libraries and dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\samue\\Downloads\\bank-additional-full.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Preliminary view of the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below histogram shows that the target value is imbalance and hence sampling technique need to be applied in the model design.\n",
    "sns.histplot(data=df, x='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # bank client data:\n",
    "   1 - age (numeric)\n",
    "   2 - job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n",
    "   3 - marital : marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n",
    "   4 - education (categorical: \"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\")\n",
    "   5 - default: has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   6 - housing: has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   7 - loan: has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
    "   8 - contact: contact communication type (categorical: \"cellular\",\"telephone\") \n",
    "   9 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "  10 - day_of_week: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n",
    "  11 - duration: last contact duration, in seconds (numeric).\n",
    "  12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "  13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "  14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "  15 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n",
    "  16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "  17 - cons.price.idx: consumer price index - monthly indicator (numeric)     \n",
    "  18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)     \n",
    "  19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "  20 - nr.employed: number of employees - quarterly indicator (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any duplicate data and drop them\n",
    "df[df.duplicated()]\n",
    "\n",
    "# Proceed to drop due to immateiral number of records (<1%)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features between catgorical and numerical\n",
    "categorical = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "numerical = ['age','duration', 'campaign','pdays','previous','emp.var.rate','cons.conf.idx','euribor3m','nr.employed']\n",
    "target = 'y'\n",
    "\n",
    "# Check the number of 'unknown variable'\n",
    "for i in categorical:\n",
    "    print(i)\n",
    "    print(len(df[df[i]=='unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the unknown job data would have lots of positive target value\n",
    "sns.histplot(data=df, x='job', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the unknown marital data would have lots of positive target value\n",
    "sns.histplot(data=df, x='marital', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown value for job and marital are immaterial, hence proposed to drop them\n",
    "# Default, housing and loan are impute as no first. For default column, it will be dropped subsequently due to limited case of default.\n",
    "# education remain as unknown category\n",
    "\n",
    "df.drop(df[(df['job'] == 'unknown') | (df['marital'] == 'unknown')].index, inplace=True)\n",
    "df['default'] = df['default'].apply(lambda x: 'no' if x == 'unknown' else x)\n",
    "df['housing'] = df['housing'].apply(lambda x: 'no' if x == 'unknown' else x)\n",
    "df['loan'] = df['loan'].apply(lambda x: 'no' if x == 'unknown' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for education\n",
    "# summarize the most frequent education level by job\n",
    "job_mapping = df.groupby('job')['education'].apply(lambda x: x.mode().iloc[0])\n",
    "# transform the summary to dictionary\n",
    "job_mapping_dict = job_mapping.to_dict()\n",
    "# apply the mapping to the unknown value\n",
    "df['education'] = df.apply(lambda row: job_mapping_dict[row['job']] if (row['education'] == 'unknown') else row['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the imputation of education level\n",
    "sns.histplot(data=df, x='education', hue='y')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the numerical details\n",
    "df[numerical].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['campaign']>10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='pdays', hue='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='previous', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duration since it is not known until the call was done\n",
    "# clear the outliers of campaign since it is not reasonable to call mutliple times in the same campaign (use 10 as a reference based on mean + 3 s.d.)\n",
    "# drop the pdays column since majority of the value is 999\n",
    "\n",
    "df.drop(['duration'], axis=1, inplace=True)\n",
    "df.drop((df[df['campaign'] > (df['campaign'].mean() + 3 * np.std(df['campaign']))]).index, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the list of numerical value\n",
    "numerical = ['campaign','previous','pdays','emp.var.rate','cons.conf.idx','euribor3m','nr.employed']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the relationship between emp.var.rate and nr.employed\n",
    "sns.lmplot(data=df, x='emp.var.rate', y='nr.employed')\n",
    "np.corrcoef(df['emp.var.rate'],df['nr.employed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high relationship between nr.employed and emp.var.rate, hence suggest to remove nr.employed to avoid duplication of features\n",
    "df.drop(['nr.employed'], axis=1, inplace=True)\n",
    "numerical = ['campaign','previous','pdays','emp.var.rate','cons.conf.idx','euribor3m']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram for categorical variable\n",
    "\n",
    "for var in categorical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=var, data=df, hue=target, palette='muted', alpha=0.7)\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Stacked Histogram for {var} based on Target')\n",
    "    plt.legend(title='Target', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis indicate that the success rate has less dependency on the day_of_week and existence of housing loan as they're distributed uniformly acorss the categorical values. Remove default columns since there is only three default cases.  Further, whether a client is default is not relevant to a deposit business.  It matters more for loan business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in numerical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=target, data=df, y=var)\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Boxplot for {var} based on Target')\n",
    "    plt.legend(title='Target', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the imbalance of target variable (11% of the data only)\n",
    "\n",
    "sns.histplot(data=df, x='y')\n",
    "(df['y']=='yes').sum() / ((df['y']=='yes').sum() + (df['y']=='no').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # More detailed exploration\n",
    "# sns.pairplot(df, hue=target)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vairous liabraries for machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the various features used for model building\n",
    "numerical = ['age','campaign','previous','pdays','cons.conf.idx','euribor3m']\n",
    "categorical = ['job','marital','education','housing','loan','contact','month','day_of_week','poutcome']\n",
    "target = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[categorical + numerical]\n",
    "y = df[target]\n",
    "\n",
    "# Transform target variable to 1 and 0 using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# build pipeline\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical),\n",
    "                                                 ('num', num_transformer, numerical)])\n",
    "\n",
    "# Combine preprocessing and SMOTE in the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=1234))  # Include SMOTE in the pipeline\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Apply the pipeline on the training data\n",
    "X_train_transformed, y_train_transformed = pipeline['smote'].fit_resample(pipeline['preprocessor'].fit_transform(X_train), y_train)\n",
    "X_test_transformed = pipeline['preprocessor'].fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_knn = {'n_neighbors': list(range(1,500)), 'weights': ['uniform', 'distance']}\n",
    "# randomsearch_knn = RandomizedSearchCV(knn_model, params_knn, cv=10, n_iter=100, scoring='recall')\n",
    "\n",
    "# randomsearch_knn.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# print(\"Best parameters from RandomSearch: \", randomsearch_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters from RandomSearch:  {'weights': 'uniform', 'n_neighbors': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vairous liabraries for machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, classification_report\n",
    "\n",
    "# Build the KNN model\n",
    "lr_model = LogisticRegression(max_iter=300)\n",
    "lr_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "svc_linear_model = SVC(kernel='linear')\n",
    "svc_linear_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svc_linear_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "svc_model = SVC(kernel='rbf', gamma=0.1)\n",
    "svc_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svc_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.01)\n",
    "gbc_model.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gbc_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform cross-validation\n",
    "# cv_scores_svc = cross_val_score(svc_linear_model, X_train_transformed, y_train_transformed, cv=5, scoring='recall')\n",
    "\n",
    "# print(\"Cross-validation scores for KNN: \", cv_scores_svc)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred_svc = svc_linear_model.predict(X_test_transformed)\n",
    "# print(\"Test accuracy for svc: \", recall_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_gbc = {'n_estimators': list(range(50,500)),'learning_rate': [0.01,0.01,0.1,0.2],'max_depth': list(range(1,5))}\n",
    "\n",
    "# randomsearch_gbc = RandomizedSearchCV(gbc_model, params_gbc, cv=5, n_iter=50, scoring='recall', random_state=1234)\n",
    "# randomsearch_gbc.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# print(\"Best parameters from RandomSearch: \", randomsearch_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is to explore the hyperparameter tunning for GBC on n_estimators, learning rate and max depth. the code are commented due to the long runtime (~3hours) and the results is pasted as below.\n",
    "\n",
    "### Best parameters from RandomSearch:  {'n_estimators': 84, 'max_depth': 4, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "gbc_model_best = GradientBoostingClassifier(n_estimators=84, max_depth=4, learning_rate=0.2)\n",
    "gbc_model_best.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gbc_model_best.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform cross-validation\n",
    "# cv_scores_gbc = cross_val_score(gbc_model, X_train_transformed, y_train_transformed, cv=5, scoring='recall')\n",
    "\n",
    "# print(\"Cross-validation scores for gbc: \", cv_scores_gbc)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred_gbc = gbc_model.predict(X_test_transformed)\n",
    "# print(\"Test accuracy for gbc: \", recall_score(y_test, y_pred_gbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform cross-validation\n",
    "# cv_scores_gbc_best = cross_val_score(gbc_model_best, X_train_transformed, y_train_transformed, cv=5, scoring='recall')\n",
    "\n",
    "# print(\"Cross-validation scores for gbc: \", cv_scores_gbc_best)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred_gbc_best = gbc_model_best.predict(X_test_transformed)\n",
    "# print(\"Test accuracy for gbc: \", recall_score(y_test, y_pred_gbc_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is to explore the hyperparameters for SVC model using polynomial kernel. The \"C\" represents the regularization parameter on the trade-off between margin and classification error.  Larger value on \"C\" represents thinner margin but lower classification error. \"Degree\" represents the degree of polynomial applied.\n",
    "\n",
    "Due to the long runtime (~two hours), this portion of code is commented and the result is extracted below.\n",
    "\n",
    "### Results: Best parameters from RandomSearch:  {'C': 4.7985894002072, 'degree': 3, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import expon\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve, classification_report\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# params_svc = {'C': expon(scale=1),\n",
    "#               'kernel': ['poly'],\n",
    "#               'degree': [2, 3, 4],  # Applicable for polynomial kernel\n",
    "#               }\n",
    "\n",
    "# svc_model_cv = SVC()\n",
    "# randomsearch_gbc = RandomizedSearchCV(svc_model_cv, params_svc, cv=5, n_iter=10, scoring='recall')\n",
    "# randomsearch_gbc.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# print(\"Best parameters from RandomSearch: \", randomsearch_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming 'X_train' and 'y_train' are your training data and labels\n",
    "svc_model_best = SVC(kernel='poly', C=4.7985894, degree=3)\n",
    "svc_model_best.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svc_best = svc_model_best.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc_best))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svc_best))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_svc_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
